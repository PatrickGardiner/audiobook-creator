# Make sure you dont add single or double quotes in the variable values, example : 
# correct format is : OPENAI_BASE_URL=http://localhost:1234/v1
# but incorrect format is : OPENAI_BASE_URL="http://localhost:1234/v1"

OPENAI_BASE_URL=<your model provider base url ex. for openai it is https://api.openai.com/v1/ or for LM Studio it is http://localhost:1234/v1>
OPENAI_API_KEY=<your model provider api key, ex. for lm-studio lm-studio>
OPENAI_MODEL_NAME=<your model name ex. qwen3-14b>
LLM_MAX_PARALLEL_REQUESTS_BATCH_SIZE=<The max number of parallel requests to make while calling the LLM. This step is used in the emotion adding flow. Make sure your inference provider supports parallel inference for example: llama-cpp with --parallel flag. You can set its value to greater than or equal to 1>
TTS_BASE_URL=<Audio generator fastapi exposed openai compatible base url ex. http://localhost:8880/v1 for both Kokoro and Orpheus TTS FastAPI>
TTS_API_KEY=<specify if needed ex. not-needed or dummy-key>
TTS_MODEL=<TTS model to use. Choose either kokoro or orpheus>
TTS_LANGUAGE=<Language code for TTS voice selection. Use: 'en' (English), 'es' (Spanish), 'fr' (French), 'it' (Italian), 'ja' (Japanese), 'pt' (Portuguese), 'zh' (Chinese), 'hi' (Hindi). This determines which voice set to use from voice_map.json. Default: 'en'>
NO_THINK_MODE=<whether to disable thinking mode in LLMs like Qwen3, R1 etc for faster inference. Takes in values of either true or false. Default is true>
TTS_MAX_PARALLEL_REQUESTS_BATCH_SIZE=<Choose the value based on this guide: https://github.com/prakharsr/audiobook-creator/?tab=readme-ov-file#parallel-batch-inferencing-of-audio-for-faster-audio-generation.